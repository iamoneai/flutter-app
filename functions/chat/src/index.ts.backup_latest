import {onRequest} from "firebase-functions/v2/https";
import {Request, Response} from "express";
import * as admin from "firebase-admin";
import Anthropic from "@anthropic-ai/sdk";
import OpenAI from "openai";
import cors from "cors";
import {v4 as uuidv4} from "uuid";

admin.initializeApp();
const db = admin.firestore();
const corsHandler = cors({origin: true});

// ============================================
// INTERFACES
// ============================================

interface ChatRequest {
  message: string;
  userId?: string;
  userName?: string;
  context?: "personal" | "work" | "family";
  provider?: string; // "all" for smart router, or specific provider
}

interface ChatResponse {
  response: string;
  provider: string;
  model: string;
  userName: string | null;
  latency: number;
  tokens: {
    input: number;
    output: number;
    total: number;
  };
  debug: {
    routingMethod: string;
    matchedCategory: string | null;
    categoryId: string | null;
    primaryLLM: string | null;
    fallbackLLM: string | null;
    usedFallback: boolean;
    restricted: string | null;
    defaultProvider: string | null;
    memoriesUsed: number;
    memoriesSaved: number;
  };
}

interface MemoryRequest {
  userId: string;
  content: string;
  type?: "fact" | "opinion" | "belief" | "episodic";
  context?: "personal" | "work" | "family";
  importance?: number;
}

interface Memory {
  id: string;
  userId: string;
  content: string;
  type: string;
  context: string;
  importance: number;
  createdAt: admin.firestore.Timestamp;
}

interface ApiKeys {
  claude?: string;
  openai?: string;
  gemini?: string;
}

interface Category {
  id: string;
  name: string;
  displayName?: string;
  keywords: string[];
  primaryLLM: string;
  fallbackLLM: string;
  priority?: number;
  isActive?: boolean;
  system_prompt?: string;
}

interface LLMConfig {
  claude?: { model: string; max_tokens: number };
  openai?: { model: string; max_tokens: number };
  gemini?: { model: string; max_tokens: number };
  default_provider?: string;
}

interface GlobalSettings {
  guardrails?: {
    blocked_domains?: string[];
    restricted_domains?: string[];
    restricted_domain_disclaimer?: string;
  };
  response?: {
    default_style?: string;
  };
}

interface InfrastructureConfig {
  vertexai?: {
    project_id?: string;
    region?: string;
    index_id?: string;
    index_endpoint_id?: string;
    deployed_index_id?: string;
    embedding_model?: string;
    enabled?: boolean;
    public_domain?: string;
  };
}

// ============================================
// LLM NAME MAPPING
// Maps category LLM names to provider and model
// ============================================

const LLM_MAPPING: Record<string, { provider: string; model: string }> = {
  // Claude models
  "claude-sonnet": { provider: "claude", model: "claude-3-5-sonnet-20241022" },
  "claude-haiku": { provider: "claude", model: "claude-3-5-haiku-20241022" },
  "claude-opus": { provider: "claude", model: "claude-3-opus-20240229" },
  // OpenAI models
  "gpt-4o": { provider: "openai", model: "gpt-4o" },
  "gpt-4o-mini": { provider: "openai", model: "gpt-4o-mini" },
  "gpt-4-turbo": { provider: "openai", model: "gpt-4-turbo" },
  "gpt-4": { provider: "openai", model: "gpt-4" },
  "gpt-3.5-turbo": { provider: "openai", model: "gpt-3.5-turbo" },
  // Gemini models
  "gemini-pro": { provider: "gemini", model: "gemini-1.5-pro" },
  "gemini-flash": { provider: "gemini", model: "gemini-2.0-flash-exp" },
  "gemini-1.5-pro": { provider: "gemini", model: "gemini-1.5-pro" },
  "gemini-1.5-flash": { provider: "gemini", model: "gemini-1.5-flash" },
  "gemini-2.0-flash-exp": { provider: "gemini", model: "gemini-2.0-flash-exp" },
};

// Normalize LLM name to provider key
function normalizeLLMName(name: string): string {
  if (!name) return "claude";
  
  // Check our mapping first
  if (LLM_MAPPING[name]) {
    return LLM_MAPPING[name].provider;
  }
  
  // Fallback detection
  const lower = name.toLowerCase();
  if (lower.includes("claude")) return "claude";
  if (lower.includes("gpt") || lower.includes("openai")) return "openai";
  if (lower.includes("gemini")) return "gemini";
  
  return "claude"; // Default
}

// Get model name from LLM name
function getModelFromLLMName(llmName: string, llmConfig: LLMConfig): string {
  // Check our mapping first
  if (LLM_MAPPING[llmName]) {
    return LLM_MAPPING[llmName].model;
  }
  
  // Fallback to config
  const provider = normalizeLLMName(llmName);
  if (provider === "claude") return llmConfig.claude?.model || "claude-3-haiku-20240307";
  if (provider === "openai") return llmConfig.openai?.model || "gpt-4o-mini";
  if (provider === "gemini") return llmConfig.gemini?.model || "gemini-2.0-flash-exp";
  
  return "claude-3-haiku-20240307";
}

// ============================================
// CONFIG LOADERS (FROM FIRESTORE)
// ============================================

async function getApiKeys(): Promise<ApiKeys> {
  try {
    const doc = await db.collection("admin").doc("api_keys").get();
    if (!doc.exists) return {};
    const data = doc.data();
    return {
      claude: data?.anthropic?.key,
      openai: data?.openai?.key,
      gemini: data?.google?.key,
    };
  } catch (error) {
    console.error("Error getting API keys:", error);
    return {};
  }
}

async function getLLMConfig(): Promise<LLMConfig> {
  try {
    const doc = await db.collection("admin").doc("config")
      .collection("settings").doc("llm").get();
    if (!doc.exists) {
      return {
        claude: { model: "claude-3-haiku-20240307", max_tokens: 1024 },
        openai: { model: "gpt-4o-mini", max_tokens: 1024 },
        gemini: { model: "gemini-2.0-flash-exp", max_tokens: 1024 },
        default_provider: "claude"
      };
    }
    return doc.data() as LLMConfig;
  } catch (error) {
    console.error("Error getting LLM config:", error);
    return {};
  }
}

async function getCategories(): Promise<Category[]> {
  try {
    const snapshot = await db.collection("admin").doc("config")
      .collection("categories").orderBy("priority").get();
    const categories = snapshot.docs
      .map((doc: admin.firestore.QueryDocumentSnapshot) => ({id: doc.id, ...doc.data()} as Category))
      .filter((c: Category) => c.isActive !== false);
    console.log(`Loaded ${categories.length} active categories`);
    return categories;
  } catch (error) {
    console.error("Error getting categories:", error);
    return [];
  }
}

async function getGlobalSettings(): Promise<GlobalSettings> {
  try {
    const doc = await db.collection("admin").doc("config")
      .collection("settings").doc("global").get();
    if (!doc.exists) return {};
    return doc.data() as GlobalSettings;
  } catch (error) {
    console.error("Error getting global settings:", error);
    return {};
  }
}

async function getInfrastructureConfig(): Promise<InfrastructureConfig> {
  try {
    const doc = await db.collection("admin").doc("config")
      .collection("settings").doc("infrastructure").get();
    if (!doc.exists) return {};
    return doc.data() as InfrastructureConfig;
  } catch (error) {
    console.error("Error getting infrastructure config:", error);
    return {};
  }
}

async function getUserProfile(uid: string): Promise<{firstName?: string; displayName?: string} | null> {
  try {
    const doc = await db.collection("users").doc(uid).get();
    if (!doc.exists) return null;
    return doc.data() as {firstName?: string; displayName?: string};
  } catch (error) {
    console.error("Error getting user profile:", error);
    return null;
  }
}

// ============================================
// SMART ROUTER - CATEGORY MATCHING
// ============================================

function matchCategory(message: string, categories: Category[]): Category | null {
  const lowerMessage = message.toLowerCase();
  
  // Categories already sorted by priority from Firestore
  for (const category of categories) {
    if (!category.keywords || !Array.isArray(category.keywords)) continue;
    for (const keyword of category.keywords) {
      if (lowerMessage.includes(keyword.toLowerCase())) {
        console.log(`Matched category: ${category.displayName || category.name} (keyword: "${keyword}")`);
        return category;
      }
    }
  }
  
  // Fallback to simple_chat if exists
  const simpleChat = categories.find((c) => c.id === "simple_chat");
  if (simpleChat) {
    console.log("No keyword match, using simple_chat category");
    return simpleChat;
  }
  
  console.log("No category matched");
  return null;
}

function checkGuardrails(message: string, settings: GlobalSettings): {
  blocked: boolean;
  restricted: boolean;
  domain: string | null;
  disclaimer: string | null;
} {
  const lowerMessage = message.toLowerCase();
  const blocked = settings.guardrails?.blocked_domains || [];
  const restricted = settings.guardrails?.restricted_domains || [];
  const disclaimer = settings.guardrails?.restricted_domain_disclaimer || "";

  console.log("Guardrails check - Blocked domains:", blocked);
  console.log("Guardrails check - Restricted domains:", restricted);
  console.log("Guardrails check - Message:", lowerMessage);

  for (const domain of blocked) {
    if (lowerMessage.includes(domain.toLowerCase())) {
      console.log(`BLOCKED: Message contains blocked domain "${domain}"`);
      return {blocked: true, restricted: false, domain, disclaimer: null};
    }
  }
  
  for (const domain of restricted) {
    if (lowerMessage.includes(domain.toLowerCase())) {
      console.log(`RESTRICTED: Message contains restricted domain "${domain}"`);
      return {
        blocked: false, 
        restricted: true, 
        domain,
        disclaimer: disclaimer.replace("{domain}", domain)
      };
    }
  }
  
  console.log("Guardrails check - PASSED (no matches)");
  return {blocked: false, restricted: false, domain: null, disclaimer: null};
}

// ============================================
// EMBEDDING FUNCTIONS
// ============================================

async function generateEmbedding(text: string, apiKey: string, model: string): Promise<number[]> {
  const url = `https://generativelanguage.googleapis.com/v1beta/models/${model}:embedContent?key=${apiKey}`;
  
  const response = await fetch(url, {
    method: "POST",
    headers: {"Content-Type": "application/json"},
    body: JSON.stringify({
      model: `models/${model}`,
      content: {parts: [{text}]},
    }),
  });

  const data: { error?: { message?: string }; embedding?: { values?: number[] } } = await response.json();
  if (data.error) {
    throw new Error(data.error.message || "Embedding API error");
  }
  
  return data.embedding?.values || [];
}

// ============================================
// MEMORY FUNCTIONS
// ============================================

async function saveMemoryToFirestore(
  memory: MemoryRequest,
  apiKey: string,
  infraConfig: InfrastructureConfig
): Promise<string> {
  const memoryId = uuidv4();
  const embeddingModel = infraConfig.vertexai?.embedding_model || "text-embedding-004";
  
  // Generate embedding
  let embedding: number[] = [];
  try {
    embedding = await generateEmbedding(memory.content, apiKey, embeddingModel);
  } catch (e) {
    console.error("Embedding generation failed:", e);
  }
  
  // Save to Firestore
  const memoryDoc = {
    id: memoryId,
    userId: memory.userId,
    content: memory.content,
    type: memory.type || "fact",
    context: memory.context || "personal",
    importance: memory.importance || 0.5,
    createdAt: admin.firestore.Timestamp.now(),
    embeddingSize: embedding.length,
  };
  
  await db.collection("users").doc(memory.userId)
    .collection("memories").doc(memoryId).set(memoryDoc);
  
  // Upsert to Vertex AI if configured
  console.log("Vector upsert check:", { enabled: infraConfig.vertexai?.enabled, embeddingLength: embedding.length });
  if (infraConfig.vertexai?.enabled && embedding.length > 0) {
    try {
      await upsertToVectorIndex(memoryId, embedding, memory.userId, infraConfig);
    } catch (e) {
      console.error("Vector upsert failed (memory still saved):", e);
    }
  }
  
  return memoryId;
}

async function upsertToVectorIndex(
  id: string,
  embedding: number[],
  userId: string,
  infraConfig: InfrastructureConfig
): Promise<void> {
  const region = infraConfig.vertexai?.region || "us-central1";
  const projectId = infraConfig.vertexai?.project_id || "app-iamoneai-c36ec";
  const indexId = infraConfig.vertexai?.index_id;

  if (!indexId) {
    console.log("Missing index_id config; skipping vector upsert");
    return;
  }

  try {
    const {GoogleAuth} = await import("google-auth-library");
    const auth = new GoogleAuth();
    const accessToken = await auth.getAccessToken();

    const url = `https://${region}-aiplatform.googleapis.com/v1/projects/${projectId}/locations/${region}/indexes/${indexId}:upsertDatapoints`;

    console.log(`Upserting datapoint ${id} to index: ${indexId}`);

    const response = await fetch(url, {
      method: "POST",
      headers: {
        "Authorization": `Bearer ${accessToken}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        datapoints: [{
          datapointId: id,
          featureVector: embedding,
          restricts: [{namespace: "userId", allowList: [userId]}],
        }],
      }),
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error("Vector upsert HTTP error:", response.status, errorText);
      throw new Error(`Vector upsert failed: ${response.status}`);
    }

    console.log(`SUCCESS: Datapoint ${id} upserted to index ${indexId}`);
  } catch (error) {
    console.error("Vector upsert error:", error);
    throw error;
  }
}

async function searchMemories(
  query: string,
  userId: string,
  apiKey: string,
  infraConfig: InfrastructureConfig,
  limit = 5
): Promise<Memory[]> {
  // If Vertex AI not configured, fall back to recent memories
  if (!infraConfig.vertexai?.enabled || !infraConfig.vertexai?.index_endpoint_id) {
    return getRecentMemories(userId, limit);
  }
  
  const embeddingModel = infraConfig.vertexai.embedding_model || "text-embedding-004";
  const queryEmbedding = await generateEmbedding(query, apiKey, embeddingModel);
  
  const publicDomain = infraConfig.vertexai.public_domain;
  const indexId = infraConfig.vertexai.deployed_index_id;
  const projectId = infraConfig.vertexai.project_id || "app-iamoneai-c36ec";
  const region = infraConfig.vertexai.region || "us-central1";
  const endpointId = infraConfig.vertexai.index_endpoint_id;
  
  if (!publicDomain || !indexId) {
    console.log("Missing public_domain or deployed_index_id, falling back to recent");
    return getRecentMemories(userId, limit);
  }
  
  try {
    const {GoogleAuth} = await import("google-auth-library");
    const auth = new GoogleAuth();
    const accessToken = await auth.getAccessToken();
    
    const url = `https://${publicDomain}/v1/projects/${projectId}/locations/${region}/indexEndpoints/${endpointId}:findNeighbors`;
    
    const response = await fetch(url, {
      method: "POST",
      headers: {
        "Authorization": `Bearer ${accessToken}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        deployed_index_id: indexId,
        queries: [{
          datapoint: { feature_vector: queryEmbedding },
          neighbor_count: limit,
        }],
      }),
    });
    
    if (!response.ok) {
      const errorText = await response.text();
      console.error("Vector search HTTP error:", response.status, errorText);
      throw new Error(`Vector search failed: ${response.status} ${errorText}`);
    }
    
    const data: { nearestNeighbors?: Array<{ neighbors?: Array<{ datapoint?: { datapointId?: string } }> }> } = await response.json();
    console.log("Vector search raw response:", JSON.stringify(data).substring(0, 500));

    const neighbors = data.nearestNeighbors?.[0]?.neighbors || [];
    const memoryIds = neighbors
      .map((n) => n.datapoint?.datapointId)
      .filter((id): id is string => Boolean(id));
    
    console.log("Vector matches found:", memoryIds.length, memoryIds);

    if (memoryIds.length === 0) {
      console.log("No vector matches, falling back to recent");
      return getRecentMemories(userId, limit);
    }
    
    // Fetch full memories from Firestore
    const memories: Memory[] = [];
    for (const memId of memoryIds) {
      const doc = await db.collection("users").doc(userId)
        .collection("memories").doc(memId).get();
      if (doc.exists) {
        memories.push(doc.data() as Memory);
      }
    }
    
    return memories;
  } catch (error) {
    console.error("Vector search error, falling back to recent:", error);
    return getRecentMemories(userId, limit);
  }
}

async function getRecentMemories(userId: string, limit: number): Promise<Memory[]> {
  const snapshot = await db.collection("users").doc(userId)
    .collection("memories")
    .orderBy("createdAt", "desc")
    .limit(limit)
    .get();
  return snapshot.docs.map((doc: admin.firestore.QueryDocumentSnapshot) => doc.data() as Memory);
}

function formatMemoriesForPrompt(memories: Memory[]): string {
  if (memories.length === 0) return "";
  
  const memoryText = memories.map((m) => 
    `[${m.type.toUpperCase()}] ${m.content}`
  ).join("\n");
  
  return `\n\n## Relevant Memories About This User:\n${memoryText}\n`;
}

// ============================================
// AUTO-MEMORY EXTRACTION
// ============================================

interface ExtractedFact {
  content: string;
  type: "fact" | "opinion" | "belief" | "episodic";
  confidence: number;
}

function extractFactsFromMessage(userMessage: string): ExtractedFact[] {
  const facts: ExtractedFact[] = [];
  const lowerMsg = userMessage.toLowerCase();
  
  // Identity - Name patterns
  const namePatterns = [
    /my name is (\w+)/i,
    /call me (\w+(?:\s+\w+)?)/i,
  ];
  
  for (const pattern of namePatterns) {
    const match = userMessage.match(pattern);
    if (match && match[1]) {
      const notNames = ["a", "the", "not", "very", "so", "here", "there"];
      if (!notNames.includes(match[1].toLowerCase())) {
        facts.push({
          content: `User's name is ${match[1]}`,
          type: "fact",
          confidence: 0.95
        });
      }
    }
  }
  
  // Preferences
  if (/i (?:really )?(?:love|like|enjoy|prefer) /i.test(lowerMsg)) {
    facts.push({ content: userMessage.trim(), type: "opinion", confidence: 0.85 });
  }
  
  // Explicit remember requests
  const rememberMatch = userMessage.match(/remember (?:that|this)[:\s]+(.+?)(?:\.|$)/i);
  if (rememberMatch && rememberMatch[1]) {
    facts.push({ content: rememberMatch[1].trim(), type: "fact", confidence: 1.0 });
  }
  
  // Location/work
  if (/i (?:live|work|am from|moved to) /i.test(lowerMsg)) {
    facts.push({ content: userMessage.trim(), type: "fact", confidence: 0.85 });
  }
  
  // Family
  if (/my (?:wife|husband|spouse|partner|son|daughter|child|mom|dad|mother|father|brother|sister)/i.test(lowerMsg)) {
    facts.push({ content: userMessage.trim(), type: "fact", confidence: 0.9 });
  }
  
  return facts;
}

async function autoSaveMemories(
  userId: string, 
  userMessage: string,
  geminiKey: string,
  infraConfig: InfrastructureConfig
): Promise<number> {
  const facts = extractFactsFromMessage(userMessage);
  console.log("Auto-save: checking message:", userMessage.substring(0, 50));
  console.log("Auto-save: extracted facts:", JSON.stringify(facts));
  let savedCount = 0;
  
  for (const fact of facts) {
    if (fact.confidence >= 0.8) {
      try {
        await saveMemoryToFirestore({
          userId,
          content: fact.content,
          type: fact.type,
          importance: fact.confidence,
          context: "personal",
        }, geminiKey, infraConfig);
        savedCount++;
        console.log(`Auto-saved memory: ${fact.content.substring(0, 50)}`);
      } catch (error) {
        console.error("Auto-save failed:", error);
      }
    }
  }
  return savedCount;
}

// ============================================
// LLM CALL FUNCTIONS
// ============================================

interface LLMResult {
  content: string;
  tokensIn: number;
  tokensOut: number;
}

async function callClaude(apiKey: string, model: string, message: string, systemPrompt: string, maxTokens: number): Promise<LLMResult> {
  const client = new Anthropic({apiKey});
  const response = await client.messages.create({
    model,
    max_tokens: maxTokens,
    system: systemPrompt,
    messages: [{role: "user", content: message}],
  });
  
  return {
    content: response.content[0].type === "text" ? response.content[0].text : "",
    tokensIn: response.usage?.input_tokens || 0,
    tokensOut: response.usage?.output_tokens || 0,
  };
}

async function callOpenAI(apiKey: string, model: string, message: string, systemPrompt: string, maxTokens: number): Promise<LLMResult> {
  const client = new OpenAI({apiKey});
  const response = await client.chat.completions.create({
    model,
    max_tokens: maxTokens,
    messages: [
      {role: "system", content: systemPrompt},
      {role: "user", content: message},
    ],
  });
  
  return {
    content: response.choices[0]?.message?.content || "",
    tokensIn: response.usage?.prompt_tokens || 0,
    tokensOut: response.usage?.completion_tokens || 0,
  };
}

async function callGemini(apiKey: string, model: string, message: string, systemPrompt: string, _maxTokens?: number): Promise<LLMResult> {
  const url = `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${apiKey}`;
  
  const response = await fetch(url, {
    method: "POST",
    headers: {"Content-Type": "application/json"},
    body: JSON.stringify({
      system_instruction: {parts: [{text: systemPrompt}]},
      contents: [{parts: [{text: message}]}],
    }),
  });
  
  const data: {
    error?: { message?: string };
    candidates?: Array<{ content?: { parts?: Array<{ text?: string }> } }>;
    usageMetadata?: { promptTokenCount?: number; candidatesTokenCount?: number };
  } = await response.json();
  if (data.error) {
    throw new Error(data.error.message || "Gemini API error");
  }
  
  return {
    content: data.candidates?.[0]?.content?.parts?.[0]?.text || "",
    tokensIn: data.usageMetadata?.promptTokenCount || 0,
    tokensOut: data.usageMetadata?.candidatesTokenCount || 0,
  };
}

// LLM Router with fallback chain
type LLMCaller = (apiKey: string, model: string, message: string, systemPrompt: string, maxTokens: number) => Promise<LLMResult>;

const LLM_CALLERS: Record<string, { call: LLMCaller; keyName: keyof ApiKeys }> = {
  claude: { call: callClaude, keyName: "claude" },
  openai: { call: callOpenAI, keyName: "openai" },
  gemini: { 
    call: (apiKey: string, model: string, message: string, systemPrompt: string, maxTokens: number) => callGemini(apiKey, model, message, systemPrompt, maxTokens),
    keyName: "gemini" 
  },
};

async function routeToLLM(
  message: string,
  category: Category | null,
  apiKeys: ApiKeys,
  systemPrompt: string,
  llmConfig: LLMConfig,
  requestedProvider?: string
): Promise<{ content: string; tokensIn: number; tokensOut: number; provider: string; model: string; usedFallback: boolean }> {
  const errors: string[] = [];
  const maxTokens = llmConfig.claude?.max_tokens || 1024;

  // Determine routing order
  let orderedProviders: string[];
  
  if (requestedProvider && requestedProvider !== "all") {
    // Direct provider request
    orderedProviders = [normalizeLLMName(requestedProvider)];
  } else if (category) {
    // Smart router - use category's primaryLLM and fallbackLLM
    const primary = normalizeLLMName(category.primaryLLM);
    const fallback = normalizeLLMName(category.fallbackLLM);
    
    orderedProviders = [primary];
    if (fallback && fallback !== primary) orderedProviders.push(fallback);
    
    // Add remaining providers as last resort
    const allProviders = ["claude", "openai", "gemini"];
    for (const p of allProviders) {
      if (!orderedProviders.includes(p)) orderedProviders.push(p);
    }
  } else {
    // No category, use default
    const defaultProvider = llmConfig.default_provider || "claude";
    orderedProviders = [normalizeLLMName(defaultProvider), "claude", "openai", "gemini"];
    // Remove duplicates
    orderedProviders = [...new Set(orderedProviders)];
  }

  console.log("Routing order:", orderedProviders.join(" -> "));

  let attemptIndex = 0;

  for (const providerName of orderedProviders) {
    const caller = LLM_CALLERS[providerName];
    if (!caller) {
      errors.push(`${providerName}: unknown provider`);
      continue;
    }
    
    const apiKey = apiKeys[caller.keyName];
    if (!apiKey) {
      errors.push(`${providerName}: no API key`);
      continue;
    }
    
    // Get the correct model for THIS provider
    let model: string;
    
    // Check if we're using category's primary or fallback
    if (attemptIndex === 0 && category?.primaryLLM && normalizeLLMName(category.primaryLLM) === providerName) {
      // Primary provider matches - use category's primary model
      model = getModelFromLLMName(category.primaryLLM, llmConfig);
    } else if (attemptIndex === 1 && category?.fallbackLLM && normalizeLLMName(category.fallbackLLM) === providerName) {
      // Fallback provider matches - use category's fallback model  
      model = getModelFromLLMName(category.fallbackLLM, llmConfig);
    } else {
      // Use provider's default model from config
      if (providerName === "claude") {
        model = llmConfig.claude?.model || "claude-3-haiku-20240307";
      } else if (providerName === "openai") {
        model = llmConfig.openai?.model || "gpt-4o-mini";
      } else if (providerName === "gemini") {
        model = llmConfig.gemini?.model || "gemini-2.0-flash-exp";
      } else {
        model = "claude-3-haiku-20240307";
      }
    }
    
    try {
      console.log(`Trying ${providerName} with model ${model}...`);
      const result = await caller.call(apiKey, model, message, systemPrompt, maxTokens);
      console.log(`${providerName} succeeded - Tokens: in=${result.tokensIn}, out=${result.tokensOut}`);
      
      return {
        ...result,
        provider: providerName,
        model,
        usedFallback: attemptIndex > 0
      };
    } catch (error: unknown) {
      const errMsg = error instanceof Error ? error.message : "Unknown error";
      console.error(`${providerName} failed:`, errMsg);
      errors.push(`${providerName}: ${errMsg}`);
      attemptIndex++;
    }
  }

  throw new Error(`All LLMs failed: ${errors.join("; ")}`);
}

// ============================================
// MAIN CHAT ENDPOINT
// ============================================

export const chat = onRequest({memory: "512MiB", timeoutSeconds: 120}, (req: Request, res: Response) => {
  corsHandler(req, res, async () => {
    if (req.method !== "POST") {
      res.status(405).json({error: "Method not allowed"});
      return;
    }

    const startTime = Date.now();

    try {
      const {message, userId, userName, context, provider: requestedProvider} = req.body as ChatRequest;

      if (!message) {
        res.status(400).json({error: "message is required"});
        return;
      }

      console.log("\n=== CHAT REQUEST ===");
      console.log(`Message: "${message}"`);
      console.log(`UserId: ${userId}`);
      console.log(`Requested provider: ${requestedProvider || "all"}`);

      // Load all configs in parallel
      const [apiKeys, llmConfig, categories, settings, infraConfig] = await Promise.all([
        getApiKeys(),
        getLLMConfig(),
        getCategories(),
        getGlobalSettings(),
        getInfrastructureConfig(),
      ]);

      console.log("Keys available:", {
        claude: !!apiKeys.claude,
        openai: !!apiKeys.openai,
        gemini: !!apiKeys.gemini,
      });
      console.log("Default provider:", llmConfig.default_provider);
      console.log("Vertex AI enabled:", infraConfig.vertexai?.enabled || false);

      // Check guardrails
      const guardrailCheck = checkGuardrails(message, settings);
      if (guardrailCheck.blocked) {
        res.json({
          response: "I cannot help with that topic. It falls outside my guidelines.",
          provider: "system",
          model: "guardrail",
          userName: null,
          latency: Date.now() - startTime,
          tokens: { input: 0, output: 0, total: 0 },
          debug: {
            routingMethod: "blocked",
            matchedCategory: null,
            categoryId: null,
            primaryLLM: null,
            fallbackLLM: null,
            usedFallback: false,
            restricted: guardrailCheck.domain,
            defaultProvider: llmConfig.default_provider || null,
            memoriesUsed: 0,
            memoriesSaved: 0,
          },
        } as ChatResponse);
        return;
      }

      // Match category (Smart Router)
      const category = matchCategory(message, categories);
      console.log("Category:", category?.id, "Primary:", category?.primaryLLM, "Fallback:", category?.fallbackLLM);

      // Get user name
      let userFirstName = userName;
      if (!userFirstName && userId) {
        const profile = await getUserProfile(userId);
        userFirstName = profile?.firstName || profile?.displayName?.split(" ")[0];
      }

      // Fetch relevant memories
      let memories: Memory[] = [];
      let memoriesText = "";
      
      if (userId && apiKeys.gemini) {
        try {
          memories = await searchMemories(message, userId, apiKeys.gemini, infraConfig, 5);
          memoriesText = formatMemoriesForPrompt(memories);
          console.log(`Found ${memories.length} relevant memories`);
        } catch (error) {
          console.error("Memory search error:", error);
        }
      }

      // Build system prompt
      const responseSettings = settings.response || {};
      const style = responseSettings.default_style || "friendly";
      let systemPrompt = "You are IAMONEAI, a personal AI guardian.";
      
      if (style === "direct") {
        systemPrompt += " Be concise and direct. Give facts without unnecessary elaboration.";
      } else if (style === "friendly") {
        systemPrompt += " Be helpful, friendly, and concise.";
      } else if (style === "conversational") {
        systemPrompt += " Be warm, engaging, and conversational. Show personality.";
      }
      
      systemPrompt += `\n\nContext: ${context || "personal"}`;
      if (userId) systemPrompt += `\nUser ID: ${userId}`;
      if (userFirstName) {
        systemPrompt += `\nUser's name: ${userFirstName}. Use their name naturally when appropriate.`;
      }
      
      // Add memories to prompt
      systemPrompt += memoriesText;
      
      // Add guardrail disclaimer if restricted
      if (guardrailCheck.restricted && guardrailCheck.disclaimer) {
        systemPrompt += `\n\nIMPORTANT: ${guardrailCheck.disclaimer}`;
      }

      // Route to LLM
      const llmResult = await routeToLLM(
        message,
        category,
        apiKeys,
        systemPrompt,
        llmConfig,
        requestedProvider
      );

      // Auto-save memories from conversation
      let memoriesSaved = 0;
      if (userId && apiKeys.gemini) {
        try {
          memoriesSaved = await autoSaveMemories(userId, message, apiKeys.gemini, infraConfig);
        } catch (e) {
          console.error("Auto-save error:", e);
        }
      }

      // Determine routing method
      let routingMethod = "smart_router";
      if (requestedProvider && requestedProvider !== "all") {
        routingMethod = "direct";
      } else if (!category) {
        routingMethod = "default";
      }

      res.json({
        response: llmResult.content,
        provider: llmResult.provider,
        model: llmResult.model,
        userName: userFirstName || null,
        latency: Date.now() - startTime,
        tokens: {
          input: llmResult.tokensIn,
          output: llmResult.tokensOut,
          total: llmResult.tokensIn + llmResult.tokensOut,
        },
        debug: {
          routingMethod,
          matchedCategory: category?.displayName || category?.name || null,
          categoryId: category?.id || null,
          primaryLLM: category?.primaryLLM || null,
          fallbackLLM: category?.fallbackLLM || null,
          usedFallback: llmResult.usedFallback,
          restricted: guardrailCheck.restricted ? guardrailCheck.domain : null,
          defaultProvider: llmConfig.default_provider || null,
          memoriesUsed: memories.length,
          memoriesSaved: memoriesSaved,
        },
      } as ChatResponse);

    } catch (error: unknown) {
      const errMsg = error instanceof Error ? error.message : "Unknown error";
      console.error("Chat error:", errMsg);
      res.status(500).json({
        error: errMsg || "Internal server error",
        latency: Date.now() - startTime,
        debug: {
          routingMethod: "error",
          matchedCategory: null,
          categoryId: null,
          primaryLLM: null,
          fallbackLLM: null,
          usedFallback: false,
          restricted: null,
          defaultProvider: null,
          memoriesUsed: 0,
          memoriesSaved: 0,
        },
      });
    }
  });
});

// ============================================
// MEMORY ENDPOINT
// ============================================

export const memory = onRequest({memory: "512MiB", timeoutSeconds: 120}, (req: Request, res: Response) => {
  corsHandler(req, res, async () => {
    if (req.method === "POST") {
      // Save new memory
      try {
        const {userId, content, type, context, importance} = req.body as MemoryRequest;

        if (!userId || !content) {
          res.status(400).json({error: "userId and content are required"});
          return;
        }

        const [apiKeys, infraConfig] = await Promise.all([
          getApiKeys(),
          getInfrastructureConfig(),
        ]);

        if (!apiKeys.gemini) {
          res.status(500).json({error: "Gemini API key required for embeddings"});
          return;
        }

        const memoryId = await saveMemoryToFirestore({
          userId,
          content,
          type: type || "fact",
          context: context || "personal",
          importance: importance || 0.5,
        }, apiKeys.gemini, infraConfig);

        res.json({
          success: true,
          memoryId,
          message: "Memory saved",
        });
      } catch (error: unknown) {
        const errMsg = error instanceof Error ? error.message : "Unknown error";
        console.error("Memory save error:", errMsg);
        res.status(500).json({error: errMsg});
      }
    } else if (req.method === "GET") {
      // Search/list memories
      try {
        const userId = req.query.userId as string;
        const query = req.query.query as string;
        const limit = parseInt(req.query.limit as string) || 10;

        if (!userId) {
          res.status(400).json({error: "userId is required"});
          return;
        }

        const [apiKeys, infraConfig] = await Promise.all([
          getApiKeys(),
          getInfrastructureConfig(),
        ]);
        
        if (query && apiKeys.gemini) {
          const memories = await searchMemories(query, userId, apiKeys.gemini, infraConfig, limit);
          res.json({memories, count: memories.length, searchType: "semantic"});
        } else {
          const memories = await getRecentMemories(userId, limit);
          res.json({memories, count: memories.length, searchType: "recent"});
        }
      } catch (error: unknown) {
        const errMsg = error instanceof Error ? error.message : "Unknown error";
        console.error("Memory fetch error:", errMsg);
        res.status(500).json({error: errMsg});
      }
    } else {
      res.status(405).json({error: "Method not allowed"});
    }
  });
});

// ============================================
// HEALTH CHECK ENDPOINT
// ============================================

export const health = onRequest({memory: "256MiB"}, (req: Request, res: Response) => {
  corsHandler(req, res, () => {
    res.json({
      status: "ok",
      timestamp: new Date().toISOString(),
      version: "2.1.0-smartrouter",
    });
  });
});

// ============================================
// USER REGISTRATION ENDPOINT
// ============================================

export const registerUser = onRequest({memory: "256MiB"}, (req: Request, res: Response) => {
  corsHandler(req, res, async () => {
    if (req.method !== "POST") {
      res.status(405).json({error: "Method not allowed"});
      return;
    }

    try {
      const {uid, email, firstName, lastName} = req.body;
      
      if (!uid || !email || !firstName || !lastName) {
        res.status(400).json({error: "Missing required fields: uid, email, firstName, lastName"});
        return;
      }

      // Generate IIN
      const now = new Date();
      const year = now.getFullYear().toString().slice(-2);
      const month = (now.getMonth() + 1).toString().padStart(2, "0");
      const randomHex = () => Math.random().toString(16).substring(2, 6).toUpperCase();
      const iin = `20AA-${year}${month}-${randomHex()}-${randomHex()}`;

      const userData = {
        iin,
        email,
        firstName,
        lastName,
        displayName: `${firstName} ${lastName}`,
        status: "ACTIVE",
        createdAt: admin.firestore.FieldValue.serverTimestamp(),
        updatedAt: admin.firestore.FieldValue.serverTimestamp(),
      };

      await db.collection("users").doc(uid).set(userData);
      
      res.json({success: true, iin, message: "User registered successfully"});
    } catch (error: unknown) {
      const errMsg = error instanceof Error ? error.message : "Unknown error";
      console.error("Registration error:", errMsg);
      res.status(500).json({error: "Failed to register user"});
    }
  });
});

// ============================================
// GET USER BY IIN ENDPOINT
// ============================================

export const getUserByIIN = onRequest({memory: "256MiB"}, (req: Request, res: Response) => {
  corsHandler(req, res, async () => {
    if (req.method !== "GET") {
      res.status(405).json({error: "Method not allowed"});
      return;
    }

    try {
      const iin = req.query.iin as string;
      if (!iin) {
        res.status(400).json({error: "IIN is required"});
        return;
      }

      const snapshot = await db.collection("users").where("iin", "==", iin).limit(1).get();
      if (snapshot.empty) {
        res.status(404).json({error: "User not found"});
        return;
      }

      const userData = snapshot.docs[0].data();
      res.json({
        success: true,
        user: {
          iin: userData.iin,
          firstName: userData.firstName,
          lastName: userData.lastName,
          displayName: userData.displayName,
          status: userData.status,
        },
      });
    } catch (error: unknown) {
      const errMsg = error instanceof Error ? error.message : "Unknown error";
      console.error("Get user error:", errMsg);
      res.status(500).json({error: "Failed to get user"});
    }
  });
});
